{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYH-pVcT3VC3"
   },
   "source": [
    "# Processing Text Part II\n",
    "\n",
    "The two fundamental deep-learning algorithms for sequence processing are **Recurrent Neural Networks** and **1D CNNs**. Applications of these algorithms include the following:\n",
    "\n",
    "- Document classification and timeseries classification, such as identifying the topic of an article or the author of a book.\n",
    "- Timeseries comparisons, such as estimating how closely related two documents are.\n",
    "- Sequence-to-sequence learning, such as translating an English sentence into French.\n",
    "- Sentiment analysis, such as classifying the sentiment of tweets or movie reviews as positive or negative.\n",
    "- Timeseries forecasting, such as predicting the future weather at a certain location given recent weather data.\n",
    "\n",
    "The following examples are modified versions of some applications that can be found in [1]. Further, the next cell is included in case you have your data on Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18817,
     "status": "ok",
     "timestamp": 1667235229868,
     "user": {
      "displayName": "Daniel Otero Fadul",
      "userId": "14832725200017864894"
     },
     "user_tz": 360
    },
    "id": "TOkJVWq73Obs",
    "outputId": "f25739e3-806c-450c-a132-a053f874aea8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%cd \"/content/drive/MyDrive/TC3007C/Sequences\"\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-cvlR1r60oL"
   },
   "source": [
    "Now we import some libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-AhM8KA61nx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras import models\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXKZiOaOF4w5"
   },
   "source": [
    "## Processing text data\n",
    "\n",
    "Deep learning for natural-language processing is pattern recognition applied to words, sentences, and paragraphs, in much the same way that computer vision is pattern recognition applied to pixels. \n",
    "\n",
    "As expected, deep-learning models do not take raw text as input, they only work with numeric tensors. To deal with this we need to **vectorize** text, which is the process of transforming text into numeric tensors. This can be done in multiple ways:\n",
    "\n",
    "- Segment text into words, and transform each word into a vector.\n",
    "- Segment text into characters, and transform each character into a vector.\n",
    "- Extract n-grams of words or characters, and transform each n-gram into a vector.\n",
    "\n",
    "The different units into which you can break down text (words, characters, or n-grams) are called **tokens**, and breaking text into such tokens is called **tokenization**. In general, text-vectorization processes consist of applying some tokenization scheme and then associating numeric vectors with the generated tokens. These vectors are fed into deep neural networks. There are multiple ways to associate a vector with a token. Let us talk about two major ones: **one-hot encoding** of tokens, and **token embedding**.\n",
    "\n",
    "### One-hot encoding\n",
    "\n",
    "One-hot encoding is the most common, most basic way to turn a token into a vector. It consists of associating a unique integer index with every word and then turning this integer index $i$ into a binary vector of size $n$, which is the size of the vocabulary: the vector is all zeros except for the i-th entry, which is equal to one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 594,
     "status": "ok",
     "timestamp": 1667235241209,
     "user": {
      "displayName": "Daniel Otero Fadul",
      "userId": "14832725200017864894"
     },
     "user_tz": 360
    },
    "id": "qJDWu5fxFQCs",
    "outputId": "604a55a5-4744-497c-f3de-77036a882df3"
   },
   "outputs": [],
   "source": [
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "tokenizer = Tokenizer(num_words=10)\n",
    "tokenizer.fit_on_texts(samples) \n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.')\n",
    "print(word_index)\n",
    "print(sequences)\n",
    "print(one_hot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwF23f-xNtN5"
   },
   "source": [
    "### Token embedding\n",
    "\n",
    "This method is mainly used for words, so it is also known as **word embeddings**. Whereas the vectors obtained through one-hot encoding are binary, sparse, and very high-dimensional (same dimensionality as the number of words in the vocabulary), word embeddings are low-dimensional floating-point vectors: that is, dense vectors, as opposed to sparse vectors. So, word embeddings pack more information into far fewer dimensions.\n",
    "\n",
    "There are two ways to obtain word embeddings: **learn word embeddings** jointly with the main task you care about, and load **pretrained word embeddings**.\n",
    "\n",
    "#### Learning word embeddings\n",
    "\n",
    "The simplest way to associate a dense vector with a word is to choose the vector at random. The problem with this approach is that the resulting embedding space has no structure. Something more useful would be to have a space in which the geometric relationships between word vectors should reflect the semantic relationships between these words. Then, word embeddings are meant to map human language into a geometric space. \n",
    "\n",
    "In real-world word-embedding spaces, common examples of meaningful geometric transformations are **gender** vectors and **plural** vectors. For instance, by adding a `female` vector to the vector `king`, we obtain the vector `queen`. By adding a `plural` vector, we obtain `kings`.Word-embedding spaces typically feature thousands of such interpretable and potentially useful vectors.\n",
    "\n",
    "Learning a word embedding of a particular task is equivalent to training an extra layer of a neural network, which is known as the `Embedding` layer. This layer can be understood as a dictionary that maps integer indices (which stand for specific words) to dense vectors. It takes integers as input, it looks up these integers in an internal dictionary, and it returns the associated vectors. By the way, this layer needs to be told, at least, the size of the vocabulary we are working with and the dimensionality of the embedding.\n",
    "\n",
    "To see how this works, let us implement a classifiers that predicts if a movie review is *positive* or *negative*. The data we will work with is the **IMDB** database that is already included in `keras`. For now, let us import the data limiting the number of the most frequent words we will handle (`max_features`), and also truncating the reviews so that they have twenty words at most (`maxlen`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6070,
     "status": "ok",
     "timestamp": 1667235252240,
     "user": {
      "displayName": "Daniel Otero Fadul",
      "userId": "14832725200017864894"
     },
     "user_tz": 360
    },
    "id": "l7kDw9PsHZxm",
    "outputId": "3fbe8943-b9ed-4f65-b516-b9eb0217446d"
   },
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = 20\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features) \n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen) \n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-g5IGgsQfwp"
   },
   "source": [
    "Let us implement a simple neural network that includes an `Embedding` layer. In this case, the dimensionality of the embedding space is eight, so the model will learn for each word a vector of eight components, which means that this layer will have 80.000 trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 592,
     "status": "ok",
     "timestamp": 1667235257161,
     "user": {
      "displayName": "Daniel Otero Fadul",
      "userId": "14832725200017864894"
     },
     "user_tz": 360
    },
    "id": "RzA_OJvtAopq",
    "outputId": "1e389895-3e77-4b80-85d8-6fb82d063385"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "                    Embedding(max_features, 8, input_length=maxlen), \n",
    "                    Flatten(),\n",
    "                    Dense(1, activation='sigmoid')\n",
    "                   ])\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc']) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMRjLKyzRGrB"
   },
   "source": [
    "And now we train..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21486,
     "status": "ok",
     "timestamp": 1667235282417,
     "user": {
      "displayName": "Daniel Otero Fadul",
      "userId": "14832725200017864894"
     },
     "user_tz": 360
    },
    "id": "Gxw6JpJADDYw",
    "outputId": "b85205c2-0626-4eee-e43e-4520f034a3af"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bz44VWdfRlcB"
   },
   "source": [
    "So we get a validation accuracy of approximately 75%, which is pretty good considering that we are only looking at the first twenty words in every review. \n",
    "\n",
    "By the way, now that the model is trained, we can see what the embedding layer learned during training. As we mentioned, the embedding layer will map every word in our corpus to a vector that lives in a space with eight dimensions. This means that a review, which has twenty words, will become a matrix in which each row is a vector of eight components.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 176,
     "status": "ok",
     "timestamp": 1667235561579,
     "user": {
      "displayName": "Daniel Otero Fadul",
      "userId": "14832725200017864894"
     },
     "user_tz": 360
    },
    "id": "XmLg2cxl9cw_",
    "outputId": "cb709b86-790c-42e3-d41e-0b3f51c9b3bf"
   },
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in model.layers[:3]] \n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "activations = activation_model.predict(np.expand_dims(X_train[0], axis=0))\n",
    "print(X_train[0])\n",
    "print(activations[0].shape)\n",
    "print(activations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnIpW_Fq8f0Q"
   },
   "source": [
    "#### Pretrained word embeddings\n",
    "\n",
    "When little training data is available, instead of learning word embeddings jointly with the problem we want to solve, we can load embedding vectors from a precomputed embedding space that we know is highly structured and exhibits useful properties. This is analogous to the concept of transfer learning: there is not enough data available to learn truly powerful features on our own, but we expect the features that we need to be fairly generic.\n",
    "\n",
    "The idea of a dense, low-dimensional embedding space for words, computed in an unsupervised way, was initially explored by Bengio et al. in the early 2000s, but it only started to take off in research and industry applications after the release of one of the most famous and successful word-embedding schemes: the **Word2vec** algorithm, developed by Tomas Mikolov at Google in 2013. Word2vec captures specific semantic properties, such as gender.\n",
    "\n",
    "There are various precomputed databases of word embeddings that you can download and use in a `keras` embedding layer. Word2vec is one of them. Another popular one is called **Global Vectors for Word Representation (GloVe)**, which was developed by Stanford researchers in 2014. This embedding technique is based on factorizing a matrix of word co-occurrence statistics. Its developers have made available precomputed embeddings for millions of English tokens, obtained from Wikipedia data and Common Crawl data.\n",
    "\n",
    "Well, let us use GloVe as a pretrained embedding. To accomplish this, we will have to work with the raw IMDB datase that can be downloaded from here: http://mng.bz/0tIo \n",
    "\n",
    "Now, let us collect the individual training reviews into a list of strings, one string per review. We will also get the review labels (positive/negative) into a labels list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNknbbpL6eAK"
   },
   "outputs": [],
   "source": [
    "imdb_dir = '/Users/dotero/Documents/TEC/Cursos/Concentracio패n IA/Mo패dulos/TC3007C/Deep Learning/Code/Sequences/IMDB'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "damHNBhGKLNA"
   },
   "source": [
    "Let us vectorize the text and prepare a training and validation split. Because pretrained word embeddings are meant to be particularly useful on problems where little training data is available, we will add the following twist: restricting the training data to the first 200 samples. In other words, we will train our model to classify movie reviews after looking at just 200 examples. Notice that we have to first shuffle the data because samples of said data are ordered (all negative first, then all positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_aM735HEH2z"
   },
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "training_samples = 200 \n",
    "validation_samples = 10000\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(f'Found {len(word_index)} unique tokens.')\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "print(f'Shape of data tensor: {data.shape}.')\n",
    "print(f'Shape of label tensor: {labels.shape[0]}.')\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "X_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "X_val = data[training_samples: training_samples + validation_samples] \n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmwL3ppMLBsj"
   },
   "source": [
    "To use the pretrained GloVe embedding from 2014 English Wikipedia we have to go to https://nlp.stanford.edu/projects/glove and download it. It is a large file, so be patient if you do not have a good internet connection. The file contains 100-dimensional embedding vectors for 400,000 words (or nonword tokens). Unzip it.\n",
    "\n",
    "Let us parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation (as number vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfmgShTXFDul"
   },
   "outputs": [],
   "source": [
    "glove_dir = '/Users/dotero/Documents/TEC/Cursos/Concentracio패n IA/Mo패dulos/TC3007C/Deep Learning/Code/Sequences/glove.6B'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print(f'Found {len(embeddings_index)} word vectors.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CENCWgOpLyp9"
   },
   "source": [
    "In order to use the parameters of the pretrained embedding, we will have to build an embedding matrix that we will feed into the embedding layer. It must be a matrix with `max_words` rows and `embedding_dim` columns, where each row $i$ contains the `embedding_dim`-dimensional vector for the word of index $i$ in the reference word index (built during tokenization). Note that index zero is not supposed to stand for any word or token, it is just a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DaYPxP2_F84O"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIP6ISciO-E4"
   },
   "source": [
    "Now we built our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85S1w8BLGkTp"
   },
   "outputs": [],
   "source": [
    "model_pre_trained = Sequential([\n",
    "                                Embedding(max_words, embedding_dim, input_length=maxlen),\n",
    "                                Flatten(),\n",
    "                                Dense(32, activation='relu'),\n",
    "                                Dense(1, activation='sigmoid')\n",
    "                               ])\n",
    "\n",
    "model_pre_trained.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhGHT2eKPGfQ"
   },
   "source": [
    "The embedding layer has a single weight matrix: a 2D float matrix where each entry $i$ is the word vector meant to be associated with index $i$. Let us load the embedding matrix that we built into the embedding layer. By the way, we do not want to alter these parameters during training, so we have to tell `keras` that the embedding layer will not be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_TNHyGzHBUy"
   },
   "outputs": [],
   "source": [
    "model_pre_trained.layers[0].set_weights([embedding_matrix])\n",
    "model_pre_trained.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZL8epgffPw9T"
   },
   "source": [
    "We are all set for compiling and training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0fcdnC6HD8q"
   },
   "outputs": [],
   "source": [
    "model_pre_trained.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model_pre_trained.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbTFxDbRP3m7"
   },
   "source": [
    "Let us visualize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ltP2t7QHafv"
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average validation accuracy: {sum(val_acc) / 10}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oqho4gwA8aE"
   },
   "source": [
    "The model quickly starts overfitting, which is unsurprising given the small number of training samples. Validation accuracy seems to reach the high fifties.\n",
    "\n",
    "Notice that results may vary because we have so few training samples, so performance is heavily dependent on exactly which 200 samples are chosen randomly. If this works poorly for you, try choosing a different random set of 200 samples, however, keep in mind that in real life we do not get to choose our training data.\n",
    "\n",
    "We can also train the same model without loading the pretrained word embeddings and without freezing the embedding layer. In that case, we will use a task-specific embedding of the input tokens, which is generally more powerful than pretrained word embeddings when lots of data is available. But in this case, we have only 200 training samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "                    Embedding(max_words, embedding_dim, input_length=maxlen),\n",
    "                    Flatten(),\n",
    "                    Dense(32, activation='relu'),\n",
    "                    Dense(1, activation='sigmoid')\n",
    "                   ])\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we see how thigs went."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average validation accuracy: {sum(val_acc) / 10}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average accuracy of the validation set with a pretrained embedding is a bit higher than without it, so, in this case, pretrained word embeddings outperform learned embeddings. If we were to increase the number of training samples, this will quickly stop being the case.\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Chollet, Francois. *Deep learning with Python*. Simon and Schuster, 2021.\n",
    "\n",
    "[2] https://code.google.com/archive/p/word2vec\n",
    "\n",
    "[3] https://nlp.stanford.edu/projects/glove"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPExAiWLPaQ5G9MY8hqCe/9",
   "collapsed_sections": [],
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
